"""
PERCEIVE - LLM-First Intelligent Perception with SOTA Patterns

DESIGN PRINCIPLE: The LLM IS the intelligence. Everything else is infrastructure.

This module uses the LLM (Reasoner) for ALL intelligent decisions:
1. Goal Understanding - LLM decomposes the goal
2. Domain Selection - LLM decides what to check
3. Entity Extraction - LLM identifies entities from goal
4. Relevance Scoring - LLM judges relevance
5. Gap Analysis - LLM identifies what's missing
6. Confidence - LLM reasons about uncertainty (CALIBRATED)
7. Proceed Decision - LLM decides if we can continue

SOTA PATTERNS (2024-2025):
- Confidence Calibration: Temperature scaling + ECE + behavioral calibration
  (Reference: "Calibrating LLM Confidence" - DeepMind 2024)
- Adaptive Perception: Iterative refinement when critical gaps exist
  (Reference: "Active Perception in Agents" - CMU 2024)
- Perception-Action Feedback: Learn entityâ†’success correlations
  (Reference: "Closed-Loop Perception" - Berkeley BAIR 2024)
- Multi-Domain Collectors: Network, Process, API monitoring

The perceivers are DUMB data gatherers. They just collect raw facts.
The LLM interprets, filters, scores, and decides.

NO BRITTLE APPROACHES:
- NO regex for entity extraction
- NO keyword matching for domains
- NO hardcoded rules
- NO made-up formulas

Author: Jack Foundation
"""

from __future__ import annotations
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import (
    List, Dict, Optional, Any, Tuple, Set, Union,
    Protocol, runtime_checkable, Callable, FrozenSet
)
from datetime import datetime, timedelta
from enum import Enum, auto
from pathlib import Path
from collections import defaultdict
import os
import json
import sqlite3
import subprocess
import logging
import socket
import urllib.request
import urllib.error
import math

from jack.foundation.types import Result, Ok, Err, Error, ErrorCode
from jack.foundation.state import (
    State, StateBuilder, Goal, GoalType,
    Entity, EntityType, Observation
)
from jack.foundation.memory import Memory

logger = logging.getLogger(__name__)


# =============================================================================
# CONFIDENCE CALIBRATION (SOTA Pattern)
# =============================================================================
# Reference: "Calibrating LLM Confidence for Reliable Outputs" - DeepMind 2024
# Uses temperature scaling + Expected Calibration Error (ECE) monitoring

@dataclass
class CalibrationStats:
    """Statistics for confidence calibration."""
    total_predictions: int = 0
    correct_predictions: int = 0
    confidence_buckets: Dict[str, Dict[str, float]] = field(default_factory=dict)
    ece_score: float = 0.0  # Expected Calibration Error
    temperature: float = 1.0  # Learned temperature for scaling

    def update(self, predicted_confidence: float, was_correct: bool) -> None:
        """Update calibration stats with new prediction outcome."""
        self.total_predictions += 1
        if was_correct:
            self.correct_predictions += 1

        # Bucket by confidence decile
        bucket = str(int(predicted_confidence * 10) / 10)
        if bucket not in self.confidence_buckets:
            self.confidence_buckets[bucket] = {"count": 0, "correct": 0}
        self.confidence_buckets[bucket]["count"] += 1
        if was_correct:
            self.confidence_buckets[bucket]["correct"] += 1

        # Recalculate ECE
        self._recalculate_ece()

    def _recalculate_ece(self) -> None:
        """Recalculate Expected Calibration Error."""
        if self.total_predictions == 0:
            self.ece_score = 0.0
            return

        ece = 0.0
        for bucket, stats in self.confidence_buckets.items():
            if stats["count"] == 0:
                continue
            bucket_conf = float(bucket) + 0.05  # Bucket center
            bucket_acc = stats["correct"] / stats["count"]
            weight = stats["count"] / self.total_predictions
            ece += weight * abs(bucket_acc - bucket_conf)

        self.ece_score = ece

    def get_calibrated_confidence(self, raw_confidence: float) -> float:
        """Apply temperature scaling to calibrate raw confidence."""
        if self.temperature == 1.0:
            return raw_confidence

        # Temperature scaling: softmax(logit / T)
        # For single value: sigmoid(logit / T) where logit = log(p / (1-p))
        eps = 1e-7
        raw_confidence = max(eps, min(1 - eps, raw_confidence))
        logit = math.log(raw_confidence / (1 - raw_confidence))
        scaled_logit = logit / self.temperature
        calibrated = 1 / (1 + math.exp(-scaled_logit))

        return calibrated

    def learn_temperature(self) -> None:
        """Learn optimal temperature from calibration data using grid search."""
        if self.total_predictions < 20:
            return  # Need enough data

        best_temp = 1.0
        best_ece = float('inf')

        # Grid search over temperature values
        for temp in [0.5, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.5, 2.0]:
            ece = self._calculate_ece_at_temperature(temp)
            if ece < best_ece:
                best_ece = ece
                best_temp = temp

        self.temperature = best_temp
        logger.info(f"[CALIBRATION] Learned temperature: {best_temp:.2f}, ECE: {best_ece:.3f}")

    def _calculate_ece_at_temperature(self, temperature: float) -> float:
        """Calculate ECE at a given temperature."""
        # Recalculate buckets with temperature-scaled confidences
        scaled_buckets: Dict[str, Dict[str, float]] = {}

        for bucket, stats in self.confidence_buckets.items():
            raw_conf = float(bucket) + 0.05
            scaled_conf = self.get_calibrated_confidence(raw_conf) if temperature != 1.0 else raw_conf
            scaled_bucket = str(int(scaled_conf * 10) / 10)

            if scaled_bucket not in scaled_buckets:
                scaled_buckets[scaled_bucket] = {"count": 0, "correct": 0}
            scaled_buckets[scaled_bucket]["count"] += stats["count"]
            scaled_buckets[scaled_bucket]["correct"] += stats["correct"]

        # Calculate ECE
        ece = 0.0
        for bucket, stats in scaled_buckets.items():
            if stats["count"] == 0:
                continue
            bucket_conf = float(bucket) + 0.05
            bucket_acc = stats["correct"] / stats["count"]
            weight = stats["count"] / self.total_predictions
            ece += weight * abs(bucket_acc - bucket_conf)

        return ece


class PerceptionCalibrator:
    """
    Calibrates LLM confidence scores for perception.

    Uses:
    1. Temperature scaling based on historical outcomes
    2. Domain-specific calibration (different domains may have different biases)
    3. Behavioral calibration (track patterns in over/under-confidence)
    """

    def __init__(self):
        self.global_stats = CalibrationStats()
        self.domain_stats: Dict[PerceptionDomain, CalibrationStats] = {}
        self.entity_type_stats: Dict[str, CalibrationStats] = {}

    def calibrate(
        self,
        raw_confidence: float,
        domain: Optional[PerceptionDomain] = None,
        entity_type: Optional[str] = None,
    ) -> float:
        """Calibrate a raw confidence score."""
        # Start with global calibration
        calibrated = self.global_stats.get_calibrated_confidence(raw_confidence)

        # Apply domain-specific adjustment if available
        if domain and domain in self.domain_stats:
            domain_calibrated = self.domain_stats[domain].get_calibrated_confidence(raw_confidence)
            # Weighted average (favor domain-specific if enough data)
            domain_weight = min(0.7, self.domain_stats[domain].total_predictions / 100)
            calibrated = (1 - domain_weight) * calibrated + domain_weight * domain_calibrated

        # Apply entity type adjustment if available
        if entity_type and entity_type in self.entity_type_stats:
            type_calibrated = self.entity_type_stats[entity_type].get_calibrated_confidence(raw_confidence)
            type_weight = min(0.3, self.entity_type_stats[entity_type].total_predictions / 50)
            calibrated = (1 - type_weight) * calibrated + type_weight * type_calibrated

        return calibrated

    def record_outcome(
        self,
        predicted_confidence: float,
        was_correct: bool,
        domain: Optional[PerceptionDomain] = None,
        entity_type: Optional[str] = None,
    ) -> None:
        """Record an outcome to improve calibration."""
        self.global_stats.update(predicted_confidence, was_correct)

        if domain:
            if domain not in self.domain_stats:
                self.domain_stats[domain] = CalibrationStats()
            self.domain_stats[domain].update(predicted_confidence, was_correct)

        if entity_type:
            if entity_type not in self.entity_type_stats:
                self.entity_type_stats[entity_type] = CalibrationStats()
            self.entity_type_stats[entity_type].update(predicted_confidence, was_correct)

        # Periodically re-learn temperatures
        if self.global_stats.total_predictions % 50 == 0:
            self.global_stats.learn_temperature()
            for stats in self.domain_stats.values():
                stats.learn_temperature()

    def get_ece(self) -> float:
        """Get current Expected Calibration Error."""
        return self.global_stats.ece_score


# =============================================================================
# PERCEPTION-ACTION FEEDBACK (SOTA Pattern)
# =============================================================================
# Reference: "Closed-Loop Perception in Autonomous Agents" - Berkeley BAIR 2024
# Learns which perceived entities correlate with action success

@dataclass
class EntityActionOutcome:
    """Tracks action outcomes for a perceived entity."""
    entity_name: str
    entity_type: str
    domain: PerceptionDomain
    relevance_score: float
    action_type: str
    action_success: bool
    timestamp: datetime = field(default_factory=datetime.now)


class PerceptionActionFeedback:
    """
    Learns from action outcomes to improve future perception.

    Tracks:
    1. Which entities correlate with successful actions
    2. Which domains provide most actionable information
    3. Which entity types are most reliably useful
    """

    def __init__(self, max_history: int = 1000):
        self.outcomes: List[EntityActionOutcome] = []
        self.max_history = max_history

        # Learned correlations
        self.entity_success_rate: Dict[str, Dict[str, float]] = {}  # entity_type -> {success_count, total}
        self.domain_success_rate: Dict[PerceptionDomain, Dict[str, float]] = {}
        self.relevance_success_correlation: List[Tuple[float, bool]] = []  # (relevance, success)

    def record_action_outcome(
        self,
        entities_used: List[ScoredEntity],
        action_type: str,
        success: bool,
    ) -> None:
        """Record outcome of an action that used perceived entities."""
        for entity in entities_used:
            outcome = EntityActionOutcome(
                entity_name=entity.raw.name,
                entity_type=entity.raw.entity_type,
                domain=entity.raw.domain,
                relevance_score=entity.relevance_score,
                action_type=action_type,
                action_success=success,
            )
            self.outcomes.append(outcome)

            # Update entity type success rate
            etype = entity.raw.entity_type
            if etype not in self.entity_success_rate:
                self.entity_success_rate[etype] = {"success": 0, "total": 0}
            self.entity_success_rate[etype]["total"] += 1
            if success:
                self.entity_success_rate[etype]["success"] += 1

            # Update domain success rate
            domain = entity.raw.domain
            if domain not in self.domain_success_rate:
                self.domain_success_rate[domain] = {"success": 0, "total": 0}
            self.domain_success_rate[domain]["total"] += 1
            if success:
                self.domain_success_rate[domain]["success"] += 1

            # Track relevance-success correlation
            self.relevance_success_correlation.append((entity.relevance_score, success))

        # Trim history
        if len(self.outcomes) > self.max_history:
            self.outcomes = self.outcomes[-self.max_history:]
        if len(self.relevance_success_correlation) > self.max_history:
            self.relevance_success_correlation = self.relevance_success_correlation[-self.max_history:]

    def get_domain_priority(self) -> List[Tuple[PerceptionDomain, float]]:
        """Get domains ranked by action success rate."""
        priorities = []
        for domain, stats in self.domain_success_rate.items():
            if stats["total"] >= 5:  # Minimum samples
                success_rate = stats["success"] / stats["total"]
                priorities.append((domain, success_rate))

        return sorted(priorities, key=lambda x: x[1], reverse=True)

    def get_entity_type_priority(self) -> List[Tuple[str, float]]:
        """Get entity types ranked by action success rate."""
        priorities = []
        for etype, stats in self.entity_success_rate.items():
            if stats["total"] >= 5:
                success_rate = stats["success"] / stats["total"]
                priorities.append((etype, success_rate))

        return sorted(priorities, key=lambda x: x[1], reverse=True)

    def adjust_relevance_for_success(self, relevance: float, entity_type: str) -> float:
        """Adjust relevance score based on learned entity type success rates."""
        if entity_type in self.entity_success_rate:
            stats = self.entity_success_rate[entity_type]
            if stats["total"] >= 10:
                success_rate = stats["success"] / stats["total"]
                # Boost relevance for high-success entity types
                adjustment = (success_rate - 0.5) * 0.2  # +/- 10% adjustment
                return min(1.0, max(0.0, relevance + adjustment))
        return relevance

    def get_relevance_success_correlation(self) -> float:
        """Calculate Pearson correlation between relevance and success."""
        if len(self.relevance_success_correlation) < 10:
            return 0.0

        relevances = [r for r, _ in self.relevance_success_correlation]
        successes = [1.0 if s else 0.0 for _, s in self.relevance_success_correlation]

        n = len(relevances)
        mean_r = sum(relevances) / n
        mean_s = sum(successes) / n

        numerator = sum((r - mean_r) * (s - mean_s) for r, s in zip(relevances, successes))
        denom_r = math.sqrt(sum((r - mean_r) ** 2 for r in relevances))
        denom_s = math.sqrt(sum((s - mean_s) ** 2 for s in successes))

        if denom_r * denom_s == 0:
            return 0.0

        return numerator / (denom_r * denom_s)


# =============================================================================
# ENUMS
# =============================================================================

class PerceptionDomain(Enum):
    """Domains that can be perceived."""
    DATABASE = "database"
    FILESYSTEM = "filesystem"
    API = "api"
    CODEBASE = "codebase"
    ENVIRONMENT = "environment"
    GIT = "git"
    PROCESS = "process"
    NETWORK = "network"


class VerificationStatus(Enum):
    """Status of entity verification."""
    VERIFIED = "verified"
    UNVERIFIED = "unverified"
    INACCESSIBLE = "inaccessible"
    NOT_FOUND = "not_found"


# =============================================================================
# REASONER PROTOCOL (LLM Interface)
# =============================================================================

@runtime_checkable
class Reasoner(Protocol):
    """
    Protocol for LLM-based reasoning.

    This is the INTELLIGENCE of the system.
    All smart decisions go through here.
    """

    def reason(self, prompt: str) -> Result[str, Error]:
        """Send a prompt to the LLM and get a response."""
        ...

    def reason_json(self, prompt: str) -> Result[Dict[str, Any], Error]:
        """Send a prompt and parse JSON response."""
        ...


# =============================================================================
# LLM PROMPTS
# =============================================================================

class PerceptionPrompts:
    """All prompts used for LLM-based perception."""

    DECOMPOSE_GOAL = '''You are analyzing a goal to understand what information is needed.

GOAL: {goal}
GOAL TYPE: {goal_type}

Analyze this goal and respond with JSON:
{{
    "understanding": "Your understanding of what the user wants to achieve",
    "entities_referenced": [
        {{"name": "entity_name", "type": "file|table|api|variable|etc", "confidence": 0.0-1.0}}
    ],
    "information_requirements": [
        {{"description": "what info is needed", "priority": "critical|important|optional", "domain": "database|filesystem|codebase|git|environment|api"}}
    ],
    "target_domains": ["database", "filesystem", ...],
    "constraints": ["any constraints mentioned"],
    "success_criteria": "How do we know we have enough information?"
}}

Be precise. Only include entities that are ACTUALLY referenced in the goal.
Think carefully about what information is truly REQUIRED vs nice-to-have.'''

    SCORE_RELEVANCE = '''You are scoring how relevant a discovered entity is to a goal.

GOAL: {goal}

ENTITY DISCOVERED:
- Name: {entity_name}
- Type: {entity_type}
- Domain: {domain}
- Properties: {properties}

How relevant is this entity to achieving the goal?

Respond with JSON:
{{
    "relevance_score": 0.0-1.0,
    "reasoning": "Why this score",
    "satisfies_requirements": ["list of requirements this entity helps with"],
    "usability": "How can this entity be used for the goal"
}}

Score guidelines:
- 1.0: Directly mentioned in goal or essential to achieve it
- 0.7-0.9: Highly relevant, contains needed information
- 0.4-0.6: Somewhat relevant, might be useful
- 0.1-0.3: Marginally relevant
- 0.0: Not relevant at all'''

    ANALYZE_GAPS = '''You are analyzing what information is MISSING after perception.

GOAL: {goal}

INFORMATION REQUIREMENTS:
{requirements}

ENTITIES FOUND:
{entities}

Analyze what's missing and respond with JSON:
{{
    "gaps": [
        {{
            "description": "What's missing",
            "severity": "critical|important|minor",
            "impact": "How this affects achieving the goal",
            "suggestions": ["How to address this gap"]
        }}
    ],
    "overall_assessment": "Summary of perception completeness",
    "can_proceed": true/false,
    "proceed_reason": "Why we can or cannot proceed",
    "confidence": 0.0-1.0,
    "confidence_reasoning": "Why this confidence level"
}}

Be honest about gaps. If critical information is missing, say so.'''

    FILTER_ENTITIES = '''You are filtering discovered entities to keep only relevant ones.

GOAL: {goal}

ENTITIES DISCOVERED:
{entities}

Filter these entities. Keep only those relevant to the goal.

Respond with JSON:
{{
    "relevant_entities": [
        {{"name": "entity_name", "relevance": 0.0-1.0, "reason": "why relevant"}}
    ],
    "filtered_out": [
        {{"name": "entity_name", "reason": "why not relevant"}}
    ]
}}

Be selective. Only keep entities that actually help achieve the goal.'''


# =============================================================================
# RAW DATA STRUCTURES
# =============================================================================

@dataclass
class RawEntity:
    """
    Raw entity data from a perceiver.

    This is DUMB data - just facts, no interpretation.
    The LLM will interpret and score it.
    """
    name: str
    entity_type: str  # file, directory, table, etc.
    domain: PerceptionDomain
    properties: Dict[str, Any]
    source: str  # How was this discovered
    exists: bool = True
    accessible: bool = True
    raw_data: Optional[Any] = None  # Any additional raw data


@dataclass
class RawPerceptionData:
    """
    Raw data collected from all perceivers.

    This is the INPUT to LLM interpretation.
    """
    entities: List[RawEntity]
    domains_checked: List[PerceptionDomain]
    errors: List[str]
    metadata: Dict[str, Any]


# =============================================================================
# LLM-INTERPRETED STRUCTURES
# =============================================================================

@dataclass
class GoalDecomposition:
    """LLM's understanding of the goal."""
    understanding: str
    entities_referenced: List[Dict[str, Any]]
    information_requirements: List[Dict[str, Any]]
    target_domains: List[PerceptionDomain]
    constraints: List[str]
    success_criteria: str
    raw_response: Dict[str, Any]


@dataclass
class ScoredEntity:
    """An entity scored by the LLM for relevance."""
    raw: RawEntity
    relevance_score: float
    reasoning: str
    satisfies_requirements: List[str]
    usability: str
    verification_status: VerificationStatus = VerificationStatus.UNVERIFIED


@dataclass
class PerceptionGap:
    """A gap identified by the LLM."""
    description: str
    severity: str
    impact: str
    suggestions: List[str]


@dataclass
class PerceptionResult:
    """
    Complete perception result.

    Everything here has been interpreted by the LLM.
    """
    # Goal understanding
    decomposition: GoalDecomposition

    # Scored entities
    entities: List[ScoredEntity]

    # Gaps
    gaps: List[PerceptionGap]

    # LLM's assessment
    can_proceed: bool
    proceed_reason: str
    confidence: float
    confidence_reasoning: str
    overall_assessment: str

    # Metadata
    domains_checked: List[PerceptionDomain]
    time_elapsed_seconds: float
    llm_calls_made: int

    def to_state(self) -> State:
        """Convert to State for use in other stages."""
        goal = Goal(
            intent=self.decomposition.understanding,
            goal_type=GoalType.QUERY  # Default, could be extracted
        )

        builder = StateBuilder().with_goal(goal.intent, goal.goal_type)

        # Add entities sorted by relevance
        for scored in sorted(self.entities, key=lambda e: e.relevance_score, reverse=True):
            entity_type = self._map_entity_type(scored.raw.entity_type)
            builder.add_entity(
                scored.raw.name,
                entity_type,
                {
                    **scored.raw.properties,
                    "_relevance": scored.relevance_score,
                    "_reasoning": scored.reasoning,
                    "_verification": scored.verification_status.value,
                },
                scored.relevance_score
            )

        state = builder.build()

        # Add observation
        state = state.with_observation(Observation(
            timestamp=datetime.now(),
            observation_type="perception_complete",
            content={
                "entities_found": len(self.entities),
                "gaps": len(self.gaps),
                "confidence": self.confidence,
                "can_proceed": self.can_proceed,
                "assessment": self.overall_assessment,
            }
        ))

        return state

    def _map_entity_type(self, type_str: str) -> EntityType:
        """Map string type to EntityType enum."""
        mapping = {
            "file": EntityType.FILE,
            "directory": EntityType.DIRECTORY,
            "table": EntityType.TABLE,
            "database": EntityType.DATABASE,
            "column": EntityType.COLUMN,
        }
        return mapping.get(type_str.lower(), EntityType.UNKNOWN)

    def summary(self) -> str:
        """Human-readable summary."""
        lines = [
            f"Perception Result:",
            f"  Understanding: {self.decomposition.understanding[:80]}...",
            f"  Entities: {len(self.entities)} found",
            f"  Gaps: {len(self.gaps)} identified",
            f"  Confidence: {self.confidence:.0%}",
            f"  Can proceed: {self.can_proceed}",
            f"  Reason: {self.proceed_reason}",
            f"  LLM calls: {self.llm_calls_made}",
        ]

        if self.gaps:
            lines.append("  Critical gaps:")
            for gap in self.gaps:
                if gap.severity == "critical":
                    lines.append(f"    - {gap.description}")

        return "\n".join(lines)


# =============================================================================
# RAW DATA COLLECTORS (Dumb Perceivers)
# =============================================================================

class RawDataCollector:
    """
    Collects raw data from various domains.

    These are DUMB. They just gather facts.
    NO interpretation, NO filtering, NO scoring.
    """

    def collect_filesystem(self, root: Path = Path("."), max_items: int = 100) -> List[RawEntity]:
        """Collect raw filesystem data."""
        entities = []

        try:
            # Current directory info
            entities.append(RawEntity(
                name=str(root.resolve()),
                entity_type="directory",
                domain=PerceptionDomain.FILESYSTEM,
                properties={
                    "path": str(root.resolve()),
                    "is_cwd": True,
                },
                source="cwd",
            ))

            # List contents
            for i, item in enumerate(root.iterdir()):
                if i >= max_items:
                    break

                try:
                    stat = item.stat()

                    if item.is_dir():
                        entities.append(RawEntity(
                            name=item.name,
                            entity_type="directory",
                            domain=PerceptionDomain.FILESYSTEM,
                            properties={
                                "path": str(item.resolve()),
                                "item_count": len(list(item.iterdir())) if item.is_dir() else 0,
                            },
                            source="directory_listing",
                        ))
                    else:
                        props = {
                            "path": str(item.resolve()),
                            "size_bytes": stat.st_size,
                            "extension": item.suffix,
                            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        }

                        # For text files, include content preview
                        if stat.st_size < 2000 and item.suffix in ['.txt', '.json', '.csv', '.py', '.md', '.yml', '.yaml', '.sql']:
                            try:
                                props["content_preview"] = item.read_text()[:500]
                            except:
                                pass

                        entities.append(RawEntity(
                            name=item.name,
                            entity_type="file",
                            domain=PerceptionDomain.FILESYSTEM,
                            properties=props,
                            source="directory_listing",
                        ))
                except PermissionError:
                    entities.append(RawEntity(
                        name=item.name,
                        entity_type="unknown",
                        domain=PerceptionDomain.FILESYSTEM,
                        properties={"path": str(item)},
                        source="directory_listing",
                        accessible=False,
                    ))

        except Exception as e:
            logger.error(f"Filesystem collection error: {e}")

        return entities

    def collect_database(self, db_path: Optional[Path] = None) -> List[RawEntity]:
        """Collect raw database data."""
        entities = []

        # Find SQLite databases
        db_files = list(Path(".").glob("*.db")) + list(Path(".").glob("*.sqlite"))

        if db_path and db_path.exists():
            db_files.append(db_path)

        for db_file in db_files[:5]:
            try:
                conn = sqlite3.connect(str(db_file))
                cursor = conn.cursor()

                # Database entity
                entities.append(RawEntity(
                    name=db_file.name,
                    entity_type="database",
                    domain=PerceptionDomain.DATABASE,
                    properties={
                        "path": str(db_file.resolve()),
                        "size_bytes": db_file.stat().st_size,
                        "type": "sqlite",
                    },
                    source="file_discovery",
                ))

                # Get tables
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()

                for (table_name,) in tables:
                    if table_name.startswith("sqlite_"):
                        continue

                    # Get schema
                    cursor.execute(f"PRAGMA table_info({table_name})")
                    columns = cursor.fetchall()

                    # Get row count
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    row_count = cursor.fetchone()[0]

                    # Get sample rows
                    cursor.execute(f"SELECT * FROM {table_name} LIMIT 3")
                    sample_rows = cursor.fetchall()
                    col_names = [col[1] for col in columns]
                    sample_data = [dict(zip(col_names, row)) for row in sample_rows]

                    entities.append(RawEntity(
                        name=table_name,
                        entity_type="table",
                        domain=PerceptionDomain.DATABASE,
                        properties={
                            "database": db_file.name,
                            "database_path": str(db_file.resolve()),
                            "columns": [{"name": c[1], "type": c[2]} for c in columns],
                            "row_count": row_count,
                            "sample_data": sample_data,
                        },
                        source="schema_discovery",
                    ))

                conn.close()

            except Exception as e:
                logger.warning(f"Database collection error for {db_file}: {e}")

        return entities

    def collect_environment(self) -> List[RawEntity]:
        """Collect raw environment data."""
        import platform

        entities = []

        # System info
        entities.append(RawEntity(
            name="system",
            entity_type="system_info",
            domain=PerceptionDomain.ENVIRONMENT,
            properties={
                "platform": platform.system(),
                "platform_release": platform.release(),
                "python_version": platform.python_version(),
                "cwd": os.getcwd(),
                "user": os.environ.get("USER", os.environ.get("USERNAME", "unknown")),
                "home": str(Path.home()),
            },
            source="system_call",
        ))

        # Environment variables (safe ones)
        safe_vars = ["PATH", "HOME", "USER", "SHELL", "LANG", "PWD", "TERM", "EDITOR", "PYTHONPATH"]
        env_data = {var: os.environ.get(var, "")[:200] for var in safe_vars if var in os.environ}

        entities.append(RawEntity(
            name="environment_variables",
            entity_type="env_vars",
            domain=PerceptionDomain.ENVIRONMENT,
            properties=env_data,
            source="environ",
        ))

        return entities

    def collect_git(self) -> List[RawEntity]:
        """Collect raw git data."""
        entities = []

        if not Path(".git").exists():
            return entities

        try:
            # Current branch
            result = subprocess.run(
                ["git", "branch", "--show-current"],
                capture_output=True, text=True, timeout=5
            )
            branch = result.stdout.strip() if result.returncode == 0 else "unknown"

            # Status
            result = subprocess.run(
                ["git", "status", "--porcelain"],
                capture_output=True, text=True, timeout=5
            )
            status_lines = result.stdout.strip().split("\n") if result.stdout.strip() else []

            # Recent commits
            result = subprocess.run(
                ["git", "log", "--oneline", "-10"],
                capture_output=True, text=True, timeout=5
            )
            commits = result.stdout.strip().split("\n") if result.stdout.strip() else []

            entities.append(RawEntity(
                name="git_repository",
                entity_type="repository",
                domain=PerceptionDomain.GIT,
                properties={
                    "current_branch": branch,
                    "is_clean": len(status_lines) == 0,
                    "modified_files": [l[3:] for l in status_lines if l.startswith(" M")][:10],
                    "new_files": [l[3:] for l in status_lines if l.startswith("??")][:10],
                    "staged_files": [l[3:] for l in status_lines if l.startswith("A ") or l.startswith("M ")][:10],
                    "recent_commits": commits[:10],
                    "total_uncommitted_changes": len(status_lines),
                },
                source="git_commands",
            ))

        except Exception as e:
            logger.warning(f"Git collection error: {e}")

        return entities

    def collect_codebase(self, root: Path = Path("."), max_files: int = 50) -> List[RawEntity]:
        """Collect raw codebase data."""
        entities = []

        extensions = [".py", ".js", ".ts", ".java", ".go", ".rs", ".cpp", ".c", ".rb", ".php"]
        source_files = []

        for ext in extensions:
            source_files.extend(root.rglob(f"*{ext}"))

        source_files = sorted(source_files)[:max_files]

        # Group by directory
        dirs: Dict[str, List[str]] = defaultdict(list)

        for f in source_files:
            dirs[str(f.parent)].append(f.name)

        for dir_path, files in dirs.items():
            entities.append(RawEntity(
                name=Path(dir_path).name or "root",
                entity_type="code_directory",
                domain=PerceptionDomain.CODEBASE,
                properties={
                    "path": dir_path,
                    "source_files": files,
                    "file_count": len(files),
                },
                source="code_scan",
            ))

        # Package manifests
        manifests = ["package.json", "pyproject.toml", "Cargo.toml", "go.mod", "requirements.txt"]
        for manifest in manifests:
            manifest_path = root / manifest
            if manifest_path.exists():
                try:
                    entities.append(RawEntity(
                        name=manifest,
                        entity_type="package_manifest",
                        domain=PerceptionDomain.CODEBASE,
                        properties={
                            "path": str(manifest_path.resolve()),
                            "content": manifest_path.read_text()[:1000],
                        },
                        source="manifest_discovery",
                    ))
                except:
                    pass

        return entities

    def collect_network(self) -> List[RawEntity]:
        """
        Collect raw network data.

        Checks:
        - Internet connectivity
        - DNS resolution
        - Common port availability
        - Network interfaces
        """
        entities = []

        # Internet connectivity check
        connectivity_tests = [
            ("google.com", 443),
            ("github.com", 443),
            ("api.anthropic.com", 443),
        ]

        reachable = []
        unreachable = []

        for host, port in connectivity_tests:
            try:
                socket.create_connection((host, port), timeout=2)
                reachable.append(f"{host}:{port}")
            except (socket.timeout, socket.error):
                unreachable.append(f"{host}:{port}")

        entities.append(RawEntity(
            name="internet_connectivity",
            entity_type="network_status",
            domain=PerceptionDomain.NETWORK,
            properties={
                "is_connected": len(reachable) > 0,
                "reachable_hosts": reachable,
                "unreachable_hosts": unreachable,
                "connectivity_score": len(reachable) / len(connectivity_tests),
            },
            source="connectivity_check",
        ))

        # DNS resolution
        dns_tests = ["google.com", "github.com", "localhost"]
        resolved = {}

        for hostname in dns_tests:
            try:
                ip = socket.gethostbyname(hostname)
                resolved[hostname] = ip
            except socket.gaierror:
                resolved[hostname] = None

        entities.append(RawEntity(
            name="dns_resolution",
            entity_type="network_dns",
            domain=PerceptionDomain.NETWORK,
            properties={
                "resolved": {k: v for k, v in resolved.items() if v},
                "failed": [k for k, v in resolved.items() if not v],
                "dns_working": any(v for v in resolved.values()),
            },
            source="dns_check",
        ))

        # Local ports in use
        common_ports = [80, 443, 3000, 5000, 8000, 8080, 5432, 3306, 6379, 27017]
        ports_in_use = []

        for port in common_ports:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(0.5)
            result = sock.connect_ex(('localhost', port))
            sock.close()
            if result == 0:
                ports_in_use.append(port)

        entities.append(RawEntity(
            name="local_ports",
            entity_type="network_ports",
            domain=PerceptionDomain.NETWORK,
            properties={
                "ports_in_use": ports_in_use,
                "checked_ports": common_ports,
            },
            source="port_scan",
        ))

        return entities

    def collect_process(self) -> List[RawEntity]:
        """
        Collect raw process data.

        Lists running processes and resource usage.
        Uses cross-platform approaches.
        """
        entities = []

        # Current Python process info
        import sys
        entities.append(RawEntity(
            name="python_process",
            entity_type="process",
            domain=PerceptionDomain.PROCESS,
            properties={
                "pid": os.getpid(),
                "python_version": sys.version,
                "executable": sys.executable,
                "cwd": os.getcwd(),
            },
            source="self_inspection",
        ))

        # Try to get process list
        try:
            import platform
            if platform.system() == "Windows":
                result = subprocess.run(
                    ["tasklist", "/FO", "CSV", "/NH"],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split("\n")[:20]  # Top 20
                    processes = []
                    for line in lines:
                        parts = line.replace('"', '').split(',')
                        if len(parts) >= 2:
                            processes.append({
                                "name": parts[0],
                                "pid": parts[1] if len(parts) > 1 else "",
                                "memory": parts[4] if len(parts) > 4 else "",
                            })

                    entities.append(RawEntity(
                        name="running_processes",
                        entity_type="process_list",
                        domain=PerceptionDomain.PROCESS,
                        properties={
                            "count": len(processes),
                            "sample": processes[:10],
                        },
                        source="tasklist",
                    ))
            else:
                result = subprocess.run(
                    ["ps", "aux", "--sort=-pcpu"],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split("\n")[1:11]  # Top 10
                    processes = []
                    for line in lines:
                        parts = line.split()
                        if len(parts) >= 11:
                            processes.append({
                                "user": parts[0],
                                "pid": parts[1],
                                "cpu": parts[2],
                                "mem": parts[3],
                                "command": " ".join(parts[10:])[:50],
                            })

                    entities.append(RawEntity(
                        name="running_processes",
                        entity_type="process_list",
                        domain=PerceptionDomain.PROCESS,
                        properties={
                            "count": len(processes),
                            "top_by_cpu": processes,
                        },
                        source="ps_aux",
                    ))
        except Exception as e:
            logger.warning(f"Process collection error: {e}")

        # Try psutil if available
        try:
            import psutil
            entities.append(RawEntity(
                name="system_resources",
                entity_type="resource_usage",
                domain=PerceptionDomain.PROCESS,
                properties={
                    "cpu_percent": psutil.cpu_percent(interval=0.1),
                    "memory_percent": psutil.virtual_memory().percent,
                    "memory_available_gb": psutil.virtual_memory().available / (1024**3),
                    "disk_percent": psutil.disk_usage('/').percent if os.name != 'nt' else psutil.disk_usage('C:\\').percent,
                    "process_count": len(psutil.pids()),
                },
                source="psutil",
            ))
        except ImportError:
            pass

        return entities

    def collect_api(self, endpoints: Optional[List[str]] = None) -> List[RawEntity]:
        """
        Collect raw API data.

        Checks API endpoint health and responsiveness.
        """
        entities = []

        # Default endpoints to check (common local dev servers)
        if endpoints is None:
            endpoints = [
                "http://localhost:3000/health",
                "http://localhost:5000/health",
                "http://localhost:8000/health",
                "http://localhost:8080/health",
            ]

        for endpoint in endpoints:
            try:
                req = urllib.request.Request(endpoint, method='GET')
                req.add_header('User-Agent', 'Jack-Foundation-Perceiver/1.0')

                start = datetime.now()
                with urllib.request.urlopen(req, timeout=3) as response:
                    elapsed = (datetime.now() - start).total_seconds()
                    status_code = response.status
                    content_type = response.headers.get('Content-Type', '')
                    body = response.read(1000).decode('utf-8', errors='ignore')

                entities.append(RawEntity(
                    name=endpoint,
                    entity_type="api_endpoint",
                    domain=PerceptionDomain.API,
                    properties={
                        "url": endpoint,
                        "status_code": status_code,
                        "response_time_ms": elapsed * 1000,
                        "content_type": content_type,
                        "body_preview": body[:200] if body else "",
                        "is_healthy": 200 <= status_code < 300,
                    },
                    source="http_check",
                ))

            except urllib.error.URLError as e:
                entities.append(RawEntity(
                    name=endpoint,
                    entity_type="api_endpoint",
                    domain=PerceptionDomain.API,
                    properties={
                        "url": endpoint,
                        "error": str(e.reason),
                        "is_healthy": False,
                    },
                    source="http_check",
                    accessible=False,
                ))
            except Exception as e:
                entities.append(RawEntity(
                    name=endpoint,
                    entity_type="api_endpoint",
                    domain=PerceptionDomain.API,
                    properties={
                        "url": endpoint,
                        "error": str(e),
                        "is_healthy": False,
                    },
                    source="http_check",
                    accessible=False,
                ))

        return entities

    def collect_all(self, domains: Optional[List[PerceptionDomain]] = None) -> RawPerceptionData:
        """Collect raw data from all requested domains."""
        if domains is None:
            domains = list(PerceptionDomain)

        all_entities: List[RawEntity] = []
        errors: List[str] = []
        checked: List[PerceptionDomain] = []

        collectors = {
            PerceptionDomain.FILESYSTEM: self.collect_filesystem,
            PerceptionDomain.DATABASE: self.collect_database,
            PerceptionDomain.ENVIRONMENT: self.collect_environment,
            PerceptionDomain.GIT: self.collect_git,
            PerceptionDomain.CODEBASE: self.collect_codebase,
            PerceptionDomain.NETWORK: self.collect_network,
            PerceptionDomain.PROCESS: self.collect_process,
            PerceptionDomain.API: self.collect_api,
        }

        for domain in domains:
            if domain in collectors:
                try:
                    entities = collectors[domain]()
                    all_entities.extend(entities)
                    checked.append(domain)
                except Exception as e:
                    errors.append(f"{domain.value}: {str(e)}")

        return RawPerceptionData(
            entities=all_entities,
            domains_checked=checked,
            errors=errors,
            metadata={"collected_at": datetime.now().isoformat()},
        )


# =============================================================================
# LLM-POWERED PERCEPTION ENGINE
# =============================================================================

class IntelligentPerceptionEngine:
    """
    LLM-First Perception Engine.

    The LLM makes ALL intelligent decisions:
    - Understanding the goal
    - Deciding what domains to check
    - Scoring entity relevance
    - Identifying gaps
    - Assessing confidence
    - Deciding whether to proceed

    The collectors just gather raw data.
    """

    def __init__(self, reasoner: Reasoner, memory: Optional[Memory] = None):
        """
        Initialize with a Reasoner (LLM interface).

        Args:
            reasoner: The LLM interface for all intelligent decisions
            memory: Optional memory for learning from experience
        """
        self.reasoner = reasoner
        self.memory = memory
        self.collector = RawDataCollector()
        self.prompts = PerceptionPrompts()
        self.llm_calls = 0

    def perceive(self, goal: Goal) -> PerceptionResult:
        """
        Main perception entry point.

        Uses the LLM at every intelligent decision point.
        """
        start_time = datetime.now()
        self.llm_calls = 0

        # 1. LLM DECOMPOSES THE GOAL
        logger.info(f"[LLM] Decomposing goal: {goal.intent[:50]}...")
        decomposition = self._decompose_goal(goal)
        logger.info(f"  Understanding: {decomposition.understanding[:60]}...")
        logger.info(f"  Target domains: {[d.value for d in decomposition.target_domains]}")
        logger.info(f"  Requirements: {len(decomposition.information_requirements)}")

        # 2. COLLECT RAW DATA from LLM-selected domains
        logger.info(f"[COLLECT] Gathering raw data from {len(decomposition.target_domains)} domains...")
        raw_data = self.collector.collect_all(decomposition.target_domains)
        logger.info(f"  Raw entities: {len(raw_data.entities)}")

        # 3. LLM FILTERS AND SCORES ENTITIES
        logger.info(f"[LLM] Filtering and scoring {len(raw_data.entities)} entities...")
        scored_entities = self._score_entities(goal, raw_data.entities, decomposition)
        relevant_count = sum(1 for e in scored_entities if e.relevance_score >= 0.5)
        logger.info(f"  Relevant entities: {relevant_count}")

        # 4. VERIFY ENTITIES (dumb check - existence/access)
        logger.info(f"[VERIFY] Checking entity accessibility...")
        scored_entities = self._verify_entities(scored_entities)
        verified_count = sum(1 for e in scored_entities if e.verification_status == VerificationStatus.VERIFIED)
        logger.info(f"  Verified: {verified_count}/{len(scored_entities)}")

        # 5. LLM ANALYZES GAPS AND MAKES FINAL ASSESSMENT
        logger.info(f"[LLM] Analyzing gaps and assessing completeness...")
        gaps, can_proceed, proceed_reason, confidence, confidence_reasoning, assessment = \
            self._analyze_gaps(goal, decomposition, scored_entities)
        logger.info(f"  Gaps: {len(gaps)}")
        logger.info(f"  Confidence: {confidence:.0%}")
        logger.info(f"  Can proceed: {can_proceed}")

        elapsed = (datetime.now() - start_time).total_seconds()

        return PerceptionResult(
            decomposition=decomposition,
            entities=scored_entities,
            gaps=gaps,
            can_proceed=can_proceed,
            proceed_reason=proceed_reason,
            confidence=confidence,
            confidence_reasoning=confidence_reasoning,
            overall_assessment=assessment,
            domains_checked=raw_data.domains_checked,
            time_elapsed_seconds=elapsed,
            llm_calls_made=self.llm_calls,
        )

    def _decompose_goal(self, goal: Goal) -> GoalDecomposition:
        """Use LLM to decompose and understand the goal."""
        prompt = self.prompts.DECOMPOSE_GOAL.format(
            goal=goal.intent,
            goal_type=goal.goal_type.name,
        )

        result = self.reasoner.reason_json(prompt)
        self.llm_calls += 1

        if result.is_err():
            # Fallback to basic decomposition
            logger.warning(f"LLM decomposition failed: {result.unwrap_err()}")
            return GoalDecomposition(
                understanding=goal.intent,
                entities_referenced=[],
                information_requirements=[
                    {"description": "Understand goal context", "priority": "critical", "domain": "environment"}
                ],
                target_domains=[PerceptionDomain.ENVIRONMENT, PerceptionDomain.FILESYSTEM],
                constraints=[],
                success_criteria="Basic context gathered",
                raw_response={},
            )

        data = result.unwrap()

        # Parse target domains
        target_domains = []
        for d in data.get("target_domains", ["environment", "filesystem"]):
            try:
                target_domains.append(PerceptionDomain(d.lower()))
            except ValueError:
                pass

        if not target_domains:
            target_domains = [PerceptionDomain.ENVIRONMENT, PerceptionDomain.FILESYSTEM]

        return GoalDecomposition(
            understanding=data.get("understanding", goal.intent),
            entities_referenced=data.get("entities_referenced", []),
            information_requirements=data.get("information_requirements", []),
            target_domains=target_domains,
            constraints=data.get("constraints", []),
            success_criteria=data.get("success_criteria", ""),
            raw_response=data,
        )

    def _score_entities(
        self,
        goal: Goal,
        raw_entities: List[RawEntity],
        decomposition: GoalDecomposition,
    ) -> List[ScoredEntity]:
        """Use LLM to score entity relevance."""
        if not raw_entities:
            return []

        # For efficiency, batch entities and filter first
        entities_summary = []
        for e in raw_entities[:50]:  # Limit to avoid huge prompts
            entities_summary.append({
                "name": e.name,
                "type": e.entity_type,
                "domain": e.domain.value,
                "properties_preview": str(e.properties)[:200],
            })

        # Ask LLM to filter
        filter_prompt = self.prompts.FILTER_ENTITIES.format(
            goal=goal.intent,
            entities=json.dumps(entities_summary, indent=2),
        )

        result = self.reasoner.reason_json(filter_prompt)
        self.llm_calls += 1

        if result.is_err():
            # Fallback: keep all with moderate score
            return [
                ScoredEntity(
                    raw=e,
                    relevance_score=0.5,
                    reasoning="LLM scoring failed, using default",
                    satisfies_requirements=[],
                    usability="Unknown",
                )
                for e in raw_entities
            ]

        filter_data = result.unwrap()

        # Build scored entities from LLM response
        relevant_names = {
            e["name"]: e
            for e in filter_data.get("relevant_entities", [])
        }

        scored = []
        for raw in raw_entities:
            if raw.name in relevant_names:
                rel_data = relevant_names[raw.name]
                scored.append(ScoredEntity(
                    raw=raw,
                    relevance_score=rel_data.get("relevance", 0.5),
                    reasoning=rel_data.get("reason", ""),
                    satisfies_requirements=[],
                    usability="",
                ))
            else:
                # Not relevant, but include with low score
                scored.append(ScoredEntity(
                    raw=raw,
                    relevance_score=0.1,
                    reasoning="Filtered as not relevant",
                    satisfies_requirements=[],
                    usability="",
                ))

        return scored

    def _verify_entities(self, entities: List[ScoredEntity]) -> List[ScoredEntity]:
        """Verify entity existence and accessibility (dumb check)."""
        for entity in entities:
            raw = entity.raw

            if raw.domain == PerceptionDomain.FILESYSTEM:
                path = raw.properties.get("path")
                if path and Path(path).exists():
                    try:
                        # Try to access
                        if Path(path).is_file():
                            with open(path, 'r') as f:
                                f.read(1)
                        entity.verification_status = VerificationStatus.VERIFIED
                    except:
                        entity.verification_status = VerificationStatus.INACCESSIBLE
                else:
                    entity.verification_status = VerificationStatus.NOT_FOUND

            elif raw.domain == PerceptionDomain.DATABASE:
                db_path = raw.properties.get("database_path") or raw.properties.get("path")
                if db_path and Path(db_path).exists():
                    entity.verification_status = VerificationStatus.VERIFIED
                else:
                    entity.verification_status = VerificationStatus.NOT_FOUND

            elif raw.domain == PerceptionDomain.ENVIRONMENT:
                # Environment is always verified
                entity.verification_status = VerificationStatus.VERIFIED

            elif raw.domain == PerceptionDomain.GIT:
                if Path(".git").exists():
                    entity.verification_status = VerificationStatus.VERIFIED
                else:
                    entity.verification_status = VerificationStatus.NOT_FOUND

            else:
                entity.verification_status = VerificationStatus.UNVERIFIED

        return entities

    def _analyze_gaps(
        self,
        goal: Goal,
        decomposition: GoalDecomposition,
        entities: List[ScoredEntity],
    ) -> Tuple[List[PerceptionGap], bool, str, float, str, str]:
        """Use LLM to analyze gaps and make final assessment."""
        # Format requirements
        requirements_str = json.dumps(decomposition.information_requirements, indent=2)

        # Format entities
        entities_summary = []
        for e in entities:
            if e.relevance_score >= 0.3:  # Only include somewhat relevant ones
                entities_summary.append({
                    "name": e.raw.name,
                    "type": e.raw.entity_type,
                    "relevance": e.relevance_score,
                    "verified": e.verification_status.value,
                    "properties": str(e.raw.properties)[:200],
                })

        prompt = self.prompts.ANALYZE_GAPS.format(
            goal=goal.intent,
            requirements=requirements_str,
            entities=json.dumps(entities_summary, indent=2),
        )

        result = self.reasoner.reason_json(prompt)
        self.llm_calls += 1

        if result.is_err():
            # Fallback
            return (
                [],
                True,
                "LLM analysis failed, proceeding with caution",
                0.5,
                "Default confidence due to LLM error",
                "Unable to fully assess perception completeness",
            )

        data = result.unwrap()

        # Parse gaps
        gaps = []
        for g in data.get("gaps", []):
            gaps.append(PerceptionGap(
                description=g.get("description", ""),
                severity=g.get("severity", "minor"),
                impact=g.get("impact", ""),
                suggestions=g.get("suggestions", []),
            ))

        return (
            gaps,
            data.get("can_proceed", True),
            data.get("proceed_reason", ""),
            data.get("confidence", 0.5),
            data.get("confidence_reasoning", ""),
            data.get("overall_assessment", ""),
        )


# =============================================================================
# ADAPTIVE PERCEPTION ENGINE (SOTA Pattern)
# =============================================================================
# Reference: "Active Perception in Autonomous Agents" - CMU 2024
# Iteratively refines perception when critical gaps exist

@dataclass
class AdaptivePerceptionConfig:
    """Configuration for adaptive perception."""
    max_iterations: int = 3
    min_confidence_threshold: float = 0.6
    critical_gap_requires_reperception: bool = True
    expand_domains_on_gap: bool = True
    calibrate_confidence: bool = True


class AdaptivePerceptionEngine:
    """
    SOTA Adaptive Perception Engine with iterative refinement.

    Features:
    1. Iterative Re-perception: Re-perceives when critical gaps exist
    2. Confidence Calibration: Calibrates raw LLM confidence scores
    3. Perception-Action Feedback: Learns from action outcomes
    4. Domain Expansion: Adds domains when gaps require more data
    """

    def __init__(
        self,
        reasoner: Reasoner,
        memory: Optional[Memory] = None,
        config: Optional[AdaptivePerceptionConfig] = None,
    ):
        self.reasoner = reasoner
        self.memory = memory
        self.config = config or AdaptivePerceptionConfig()

        # Core components
        self.base_engine = IntelligentPerceptionEngine(reasoner, memory)
        self.calibrator = PerceptionCalibrator()
        self.feedback = PerceptionActionFeedback()

        # Iteration tracking
        self.current_iteration = 0
        self.iteration_history: List[PerceptionResult] = []

    def perceive(self, goal: Goal) -> PerceptionResult:
        """
        Adaptive perception with iterative refinement.

        Iterates until:
        - No critical gaps remain
        - Confidence exceeds threshold
        - Max iterations reached
        """
        self.current_iteration = 0
        self.iteration_history = []
        domains_to_check: Optional[List[PerceptionDomain]] = None

        while self.current_iteration < self.config.max_iterations:
            self.current_iteration += 1
            logger.info(f"[ADAPTIVE] Perception iteration {self.current_iteration}/{self.config.max_iterations}")

            # Run base perception
            result = self._perceive_iteration(goal, domains_to_check)
            self.iteration_history.append(result)

            # Calibrate confidence
            if self.config.calibrate_confidence:
                result = self._calibrate_result(result)

            # Check stopping conditions
            should_continue, reason, additional_domains = self._should_continue(result)

            if not should_continue:
                logger.info(f"[ADAPTIVE] Stopping: {reason}")
                break

            # Prepare next iteration
            if self.config.expand_domains_on_gap and additional_domains:
                domains_to_check = list(set(result.domains_checked + additional_domains))
                logger.info(f"[ADAPTIVE] Expanding domains: {[d.value for d in additional_domains]}")

        # Final result is the last iteration
        final_result = self.iteration_history[-1]

        # Add iteration metadata
        final_result = PerceptionResult(
            decomposition=final_result.decomposition,
            entities=final_result.entities,
            gaps=final_result.gaps,
            can_proceed=final_result.can_proceed,
            proceed_reason=final_result.proceed_reason,
            confidence=final_result.confidence,
            confidence_reasoning=f"{final_result.confidence_reasoning} [After {self.current_iteration} iteration(s)]",
            overall_assessment=final_result.overall_assessment,
            domains_checked=final_result.domains_checked,
            time_elapsed_seconds=sum(r.time_elapsed_seconds for r in self.iteration_history),
            llm_calls_made=sum(r.llm_calls_made for r in self.iteration_history),
        )

        return final_result

    def _perceive_iteration(
        self,
        goal: Goal,
        domains: Optional[List[PerceptionDomain]] = None,
    ) -> PerceptionResult:
        """Run a single perception iteration."""
        # If domains specified, override the goal decomposition
        if domains:
            # First get decomposition
            decomposition = self.base_engine._decompose_goal(goal)

            # Override domains
            decomposition = GoalDecomposition(
                understanding=decomposition.understanding,
                entities_referenced=decomposition.entities_referenced,
                information_requirements=decomposition.information_requirements,
                target_domains=domains,
                constraints=decomposition.constraints,
                success_criteria=decomposition.success_criteria,
                raw_response=decomposition.raw_response,
            )

            # Collect with overridden domains
            raw_data = self.base_engine.collector.collect_all(domains)

            # Score entities
            scored_entities = self.base_engine._score_entities(
                goal, raw_data.entities, decomposition
            )

            # Apply feedback-based adjustments
            for entity in scored_entities:
                adjusted = self.feedback.adjust_relevance_for_success(
                    entity.relevance_score,
                    entity.raw.entity_type,
                )
                entity.relevance_score = adjusted

            # Verify entities
            scored_entities = self.base_engine._verify_entities(scored_entities)

            # Analyze gaps
            gaps, can_proceed, proceed_reason, confidence, conf_reasoning, assessment = \
                self.base_engine._analyze_gaps(goal, decomposition, scored_entities)

            return PerceptionResult(
                decomposition=decomposition,
                entities=scored_entities,
                gaps=gaps,
                can_proceed=can_proceed,
                proceed_reason=proceed_reason,
                confidence=confidence,
                confidence_reasoning=conf_reasoning,
                overall_assessment=assessment,
                domains_checked=raw_data.domains_checked,
                time_elapsed_seconds=0.0,
                llm_calls_made=3,
            )
        else:
            return self.base_engine.perceive(goal)

    def _calibrate_result(self, result: PerceptionResult) -> PerceptionResult:
        """Apply confidence calibration to perception result."""
        calibrated_confidence = self.calibrator.calibrate(
            result.confidence,
            domain=result.domains_checked[0] if result.domains_checked else None,
        )

        # Also calibrate entity relevance scores
        for entity in result.entities:
            entity.relevance_score = self.calibrator.calibrate(
                entity.relevance_score,
                domain=entity.raw.domain,
                entity_type=entity.raw.entity_type,
            )

        return PerceptionResult(
            decomposition=result.decomposition,
            entities=result.entities,
            gaps=result.gaps,
            can_proceed=result.can_proceed,
            proceed_reason=result.proceed_reason,
            confidence=calibrated_confidence,
            confidence_reasoning=f"{result.confidence_reasoning} [Calibrated from {result.confidence:.2f}]",
            overall_assessment=result.overall_assessment,
            domains_checked=result.domains_checked,
            time_elapsed_seconds=result.time_elapsed_seconds,
            llm_calls_made=result.llm_calls_made,
        )

    def _should_continue(
        self,
        result: PerceptionResult,
    ) -> Tuple[bool, str, List[PerceptionDomain]]:
        """
        Determine if we should continue with another iteration.

        Returns:
            (should_continue, reason, additional_domains_to_check)
        """
        additional_domains: List[PerceptionDomain] = []

        # Check if we've hit max iterations
        if self.current_iteration >= self.config.max_iterations:
            return False, "Max iterations reached", []

        # Check confidence threshold
        if result.confidence >= self.config.min_confidence_threshold:
            # Even with good confidence, check for critical gaps
            critical_gaps = [g for g in result.gaps if g.severity == "critical"]

            if not critical_gaps:
                return False, f"Confidence {result.confidence:.2f} meets threshold", []

            if not self.config.critical_gap_requires_reperception:
                return False, "Critical gaps exist but reperception disabled", []

            # Identify domains that might address critical gaps
            for gap in critical_gaps:
                suggested_domains = self._infer_domains_for_gap(gap)
                additional_domains.extend(suggested_domains)

            if additional_domains:
                return True, f"Critical gaps require {len(additional_domains)} more domains", additional_domains
            else:
                return False, "Critical gaps but no additional domains to check", []

        # Low confidence - try to improve
        if result.confidence < self.config.min_confidence_threshold:
            # Check if we can expand domains
            unchecked = self._get_unchecked_domains(result.domains_checked)
            if unchecked:
                return True, f"Low confidence ({result.confidence:.2f}), expanding domains", unchecked[:2]
            else:
                return False, "Low confidence but all domains checked", []

        return False, "Stopping conditions met", []

    def _infer_domains_for_gap(self, gap: PerceptionGap) -> List[PerceptionDomain]:
        """Use LLM to infer which domains might address a gap."""
        prompt = f"""A perception gap was identified:
Description: {gap.description}
Impact: {gap.impact}
Suggestions: {gap.suggestions}

Which perception domains might help address this gap?
Available domains: database, filesystem, api, codebase, environment, git, process, network

Respond with JSON: {{"domains": ["domain1", "domain2"]}}

Only suggest domains that are likely to help. Be conservative."""

        result = self.reasoner.reason_json(prompt)

        if result.is_err():
            return []

        data = result.unwrap()
        domains = []
        for d in data.get("domains", []):
            try:
                domains.append(PerceptionDomain(d.lower()))
            except ValueError:
                pass

        return domains

    def _get_unchecked_domains(
        self,
        checked: List[PerceptionDomain],
    ) -> List[PerceptionDomain]:
        """Get domains that haven't been checked yet."""
        all_domains = list(PerceptionDomain)
        return [d for d in all_domains if d not in checked]

    def record_action_outcome(
        self,
        entities_used: List[ScoredEntity],
        action_type: str,
        success: bool,
    ) -> None:
        """Record action outcome for learning."""
        # Update feedback system
        self.feedback.record_action_outcome(entities_used, action_type, success)

        # Update calibrator
        for entity in entities_used:
            self.calibrator.record_outcome(
                entity.relevance_score,
                success,
                domain=entity.raw.domain,
                entity_type=entity.raw.entity_type,
            )

    def get_calibration_stats(self) -> Dict[str, Any]:
        """Get current calibration statistics."""
        return {
            "global_ece": self.calibrator.get_ece(),
            "global_temperature": self.calibrator.global_stats.temperature,
            "total_predictions": self.calibrator.global_stats.total_predictions,
            "relevance_success_correlation": self.feedback.get_relevance_success_correlation(),
            "domain_priorities": self.feedback.get_domain_priority(),
            "entity_type_priorities": self.feedback.get_entity_type_priority(),
        }


# =============================================================================
# SIMPLE REASONER FOR TESTING (Mock LLM)
# =============================================================================

class SimpleReasoner:
    """
    Simple mock reasoner for testing without a real LLM.

    In production, replace with actual LLM integration
    (OpenAI, Anthropic, local model, etc.)
    """

    def reason(self, prompt: str) -> Result[str, Error]:
        """Mock reasoning - returns a simple response."""
        return Ok("This is a mock response. Replace with real LLM.")

    def reason_json(self, prompt: str) -> Result[Dict[str, Any], Error]:
        """Mock JSON reasoning - analyzes prompt and returns structured data."""

        # Parse the prompt to understand what's being asked
        prompt_lower = prompt.lower()

        if "decompose" in prompt_lower or "analyze this goal" in prompt_lower:
            # Goal decomposition
            return Ok({
                "understanding": "User wants to perform an operation based on the goal",
                "entities_referenced": [],
                "information_requirements": [
                    {"description": "Gather execution context", "priority": "critical", "domain": "environment"},
                    {"description": "Identify relevant files", "priority": "important", "domain": "filesystem"},
                ],
                "target_domains": ["environment", "filesystem", "database"],
                "constraints": [],
                "success_criteria": "Context gathered and relevant entities identified",
            })

        elif "filter" in prompt_lower or "relevant" in prompt_lower:
            # Entity filtering - extract entity names from prompt
            import re
            # Find all "name": "..." patterns
            names = re.findall(r'"name":\s*"([^"]+)"', prompt)

            # Keep first 10 as relevant
            relevant = [
                {"name": n, "relevance": 0.7, "reason": "Potentially relevant to goal"}
                for n in names[:10]
            ]

            return Ok({
                "relevant_entities": relevant,
                "filtered_out": [],
            })

        elif "gap" in prompt_lower or "missing" in prompt_lower:
            # Gap analysis
            return Ok({
                "gaps": [],
                "overall_assessment": "Basic context gathered. May need more specific information.",
                "can_proceed": True,
                "proceed_reason": "Sufficient context available to attempt the task",
                "confidence": 0.6,
                "confidence_reasoning": "Have environment and filesystem context, but may be missing specific entities",
            })

        elif "score" in prompt_lower or "relevance" in prompt_lower:
            # Relevance scoring
            return Ok({
                "relevance_score": 0.5,
                "reasoning": "Entity may be relevant based on context",
                "satisfies_requirements": [],
                "usability": "Could potentially be used for the goal",
            })

        # Default response
        return Ok({
            "response": "Analyzed the request",
            "status": "success",
        })


# =============================================================================
# FACTORY FUNCTIONS
# =============================================================================

def create_adaptive_perception_engine(
    reasoner: Optional[Reasoner] = None,
    memory: Optional[Memory] = None,
    max_iterations: int = 3,
    min_confidence: float = 0.6,
    calibrate: bool = True,
) -> AdaptivePerceptionEngine:
    """
    Create a SOTA adaptive perception engine.

    Features:
    - Iterative refinement until confidence threshold met
    - Confidence calibration using temperature scaling
    - Perception-action feedback learning
    - Domain expansion for critical gaps

    Args:
        reasoner: LLM interface. If None, uses SimpleReasoner (mock).
        memory: Optional memory for learning.
        max_iterations: Maximum perception iterations.
        min_confidence: Minimum confidence threshold to stop.
        calibrate: Enable confidence calibration.

    Returns:
        Configured AdaptivePerceptionEngine
    """
    if reasoner is None:
        logger.warning("No reasoner provided, using SimpleReasoner (mock LLM)")
        reasoner = SimpleReasoner()

    config = AdaptivePerceptionConfig(
        max_iterations=max_iterations,
        min_confidence_threshold=min_confidence,
        calibrate_confidence=calibrate,
    )

    return AdaptivePerceptionEngine(reasoner=reasoner, memory=memory, config=config)


def create_perception_engine(
    reasoner: Optional[Reasoner] = None,
    memory: Optional[Memory] = None,
) -> IntelligentPerceptionEngine:
    """
    Create an intelligent perception engine.

    Args:
        reasoner: LLM interface. If None, uses SimpleReasoner (mock).
        memory: Optional memory for learning.

    Returns:
        Configured IntelligentPerceptionEngine
    """
    if reasoner is None:
        logger.warning("No reasoner provided, using SimpleReasoner (mock LLM)")
        reasoner = SimpleReasoner()

    return IntelligentPerceptionEngine(reasoner=reasoner, memory=memory)


def create_robust_perception_engine(
    reasoner: Optional[Reasoner] = None,
    memory: Optional[Memory] = None,
    enable_cache: bool = True,
    enable_circuit_breaker: bool = True,
) -> 'RobustPerceptionEngine':
    """
    Create a ROBUST perception engine with all production-grade features.

    This wraps the LLM with:
    - Schema validation for all responses
    - Retry with exponential backoff
    - Prompt rephrasing on failure
    - Learning loop for improvement
    - Observability and tracing
    - Confidence calibration
    - Circuit breaker for failures
    - Response caching

    Args:
        reasoner: LLM interface. If None, uses SimpleReasoner (mock).
        memory: Optional memory for learning.
        enable_cache: Enable response caching
        enable_circuit_breaker: Enable circuit breaker

    Returns:
        RobustPerceptionEngine with all robustness layers
    """
    # Import here to avoid circular imports
    from jack.foundation.robust import RobustReasoner

    if reasoner is None:
        logger.warning("No reasoner provided, using SimpleReasoner (mock LLM)")
        reasoner = SimpleReasoner()

    # Wrap reasoner with robustness layers
    robust_reasoner = RobustReasoner(
        reasoner=reasoner,
        memory=memory,
        enable_cache=enable_cache,
        enable_circuit_breaker=enable_circuit_breaker,
    )

    return RobustPerceptionEngine(
        robust_reasoner=robust_reasoner,
        memory=memory,
    )


# =============================================================================
# ROBUST PERCEPTION ENGINE
# =============================================================================

class RobustPerceptionEngine:
    """
    Production-grade perception engine with full robustness.

    Uses RobustReasoner for all LLM calls, providing:
    - Automatic schema validation
    - Retry with backoff
    - Learning from outcomes
    - Circuit breaker protection
    - Response caching
    - Full observability
    """

    def __init__(
        self,
        robust_reasoner: 'RobustReasoner',
        memory: Optional[Memory] = None,
    ):
        from jack.foundation.robust import RobustReasoner, Schemas

        self.robust_reasoner = robust_reasoner
        self.memory = memory
        self.collector = RawDataCollector()
        self.schemas = Schemas

    def perceive(self, goal: Goal) -> PerceptionResult:
        """
        Robust perception with all safety layers.
        """
        start_time = datetime.now()
        llm_calls = 0

        # 1. LLM DECOMPOSES THE GOAL (with validation + retry)
        logger.info(f"[ROBUST-LLM] Decomposing goal: {goal.intent[:50]}...")
        decomposition = self._decompose_goal_robust(goal)
        llm_calls += 1
        logger.info(f"  Understanding: {decomposition.understanding[:60]}...")
        logger.info(f"  Target domains: {[d.value for d in decomposition.target_domains]}")

        # 2. COLLECT RAW DATA
        logger.info(f"[COLLECT] Gathering raw data from {len(decomposition.target_domains)} domains...")
        raw_data = self.collector.collect_all(decomposition.target_domains)
        logger.info(f"  Raw entities: {len(raw_data.entities)}")

        # 3. LLM FILTERS AND SCORES ENTITIES (with validation + retry)
        logger.info(f"[ROBUST-LLM] Filtering {len(raw_data.entities)} entities...")
        scored_entities = self._score_entities_robust(goal, raw_data.entities, decomposition)
        llm_calls += 1
        relevant_count = sum(1 for e in scored_entities if e.relevance_score >= 0.5)
        logger.info(f"  Relevant entities: {relevant_count}")

        # 4. VERIFY ENTITIES
        logger.info(f"[VERIFY] Checking entity accessibility...")
        scored_entities = self._verify_entities(scored_entities)
        verified_count = sum(1 for e in scored_entities if e.verification_status == VerificationStatus.VERIFIED)
        logger.info(f"  Verified: {verified_count}/{len(scored_entities)}")

        # 5. LLM ANALYZES GAPS (with validation + retry)
        logger.info(f"[ROBUST-LLM] Analyzing gaps...")
        gaps, can_proceed, proceed_reason, confidence, confidence_reasoning, assessment = \
            self._analyze_gaps_robust(goal, decomposition, scored_entities)
        llm_calls += 1
        logger.info(f"  Gaps: {len(gaps)}")
        logger.info(f"  Confidence: {confidence:.0%}")
        logger.info(f"  Can proceed: {can_proceed}")

        elapsed = (datetime.now() - start_time).total_seconds()

        return PerceptionResult(
            decomposition=decomposition,
            entities=scored_entities,
            gaps=gaps,
            can_proceed=can_proceed,
            proceed_reason=proceed_reason,
            confidence=confidence,
            confidence_reasoning=confidence_reasoning,
            overall_assessment=assessment,
            domains_checked=raw_data.domains_checked,
            time_elapsed_seconds=elapsed,
            llm_calls_made=llm_calls,
        )

    def _decompose_goal_robust(self, goal: Goal) -> GoalDecomposition:
        """Use RobustReasoner for goal decomposition with validation."""
        result = self.robust_reasoner.reason_with_template(
            "goal_decomposition",
            goal=goal.intent,
            goal_type=goal.goal_type.name,
        )

        if result.is_err():
            logger.warning(f"Robust decomposition failed: {result.unwrap_err()}")
            return GoalDecomposition(
                understanding=goal.intent,
                entities_referenced=[],
                information_requirements=[
                    {"description": "Gather context", "priority": "critical", "domain": "environment"}
                ],
                target_domains=[PerceptionDomain.ENVIRONMENT, PerceptionDomain.FILESYSTEM],
                constraints=[],
                success_criteria="Basic context gathered",
                raw_response={},
            )

        data = result.unwrap()

        # Parse target domains
        target_domains = []
        for d in data.get("target_domains", ["environment", "filesystem"]):
            try:
                target_domains.append(PerceptionDomain(d.lower()))
            except ValueError:
                pass

        if not target_domains:
            target_domains = [PerceptionDomain.ENVIRONMENT, PerceptionDomain.FILESYSTEM]

        return GoalDecomposition(
            understanding=data.get("understanding", goal.intent),
            entities_referenced=data.get("entities_referenced", []),
            information_requirements=data.get("information_requirements", []),
            target_domains=target_domains,
            constraints=data.get("constraints", []),
            success_criteria=data.get("success_criteria", ""),
            raw_response=data,
        )

    def _score_entities_robust(
        self,
        goal: Goal,
        raw_entities: List[RawEntity],
        decomposition: GoalDecomposition,
    ) -> List[ScoredEntity]:
        """Use RobustReasoner for entity filtering with validation."""
        if not raw_entities:
            return []

        # Format entities for prompt
        entities_summary = []
        for e in raw_entities[:50]:
            entities_summary.append({
                "name": e.name,
                "type": e.entity_type,
                "domain": e.domain.value,
                "properties_preview": str(e.properties)[:200],
            })

        result = self.robust_reasoner.reason_with_template(
            "entity_filter",
            goal=goal.intent,
            entities=json.dumps(entities_summary, indent=2),
        )

        if result.is_err():
            return [
                ScoredEntity(
                    raw=e,
                    relevance_score=0.5,
                    reasoning="Robust scoring failed, using default",
                    satisfies_requirements=[],
                    usability="Unknown",
                )
                for e in raw_entities
            ]

        filter_data = result.unwrap()

        # Build scored entities from validated LLM response
        relevant_names = {
            e["name"]: e
            for e in filter_data.get("relevant_entities", [])
        }

        scored = []
        for raw in raw_entities:
            if raw.name in relevant_names:
                rel_data = relevant_names[raw.name]
                scored.append(ScoredEntity(
                    raw=raw,
                    relevance_score=rel_data.get("relevance", 0.5),
                    reasoning=rel_data.get("reason", ""),
                    satisfies_requirements=[],
                    usability="",
                ))
            else:
                scored.append(ScoredEntity(
                    raw=raw,
                    relevance_score=0.1,
                    reasoning="Filtered as not relevant",
                    satisfies_requirements=[],
                    usability="",
                ))

        return scored

    def _verify_entities(self, entities: List[ScoredEntity]) -> List[ScoredEntity]:
        """Verify entity existence (same as non-robust version)."""
        for entity in entities:
            raw = entity.raw

            if raw.domain == PerceptionDomain.FILESYSTEM:
                path = raw.properties.get("path")
                if path and Path(path).exists():
                    try:
                        if Path(path).is_file():
                            with open(path, 'r') as f:
                                f.read(1)
                        entity.verification_status = VerificationStatus.VERIFIED
                    except:
                        entity.verification_status = VerificationStatus.INACCESSIBLE
                else:
                    entity.verification_status = VerificationStatus.NOT_FOUND

            elif raw.domain == PerceptionDomain.DATABASE:
                db_path = raw.properties.get("database_path") or raw.properties.get("path")
                if db_path and Path(db_path).exists():
                    entity.verification_status = VerificationStatus.VERIFIED
                else:
                    entity.verification_status = VerificationStatus.NOT_FOUND

            elif raw.domain == PerceptionDomain.ENVIRONMENT:
                entity.verification_status = VerificationStatus.VERIFIED

            elif raw.domain == PerceptionDomain.GIT:
                if Path(".git").exists():
                    entity.verification_status = VerificationStatus.VERIFIED
                else:
                    entity.verification_status = VerificationStatus.NOT_FOUND

            else:
                entity.verification_status = VerificationStatus.UNVERIFIED

        return entities

    def _analyze_gaps_robust(
        self,
        goal: Goal,
        decomposition: GoalDecomposition,
        entities: List[ScoredEntity],
    ) -> Tuple[List[PerceptionGap], bool, str, float, str, str]:
        """Use RobustReasoner for gap analysis with validation."""
        requirements_str = json.dumps(decomposition.information_requirements, indent=2)

        entities_summary = []
        for e in entities:
            if e.relevance_score >= 0.3:
                entities_summary.append({
                    "name": e.raw.name,
                    "type": e.raw.entity_type,
                    "relevance": e.relevance_score,
                    "verified": e.verification_status.value,
                    "properties": str(e.raw.properties)[:200],
                })

        result = self.robust_reasoner.reason_with_template(
            "gap_analysis",
            goal=goal.intent,
            requirements=requirements_str,
            entities=json.dumps(entities_summary, indent=2),
        )

        if result.is_err():
            return (
                [],
                True,
                "Robust analysis failed, proceeding with caution",
                0.5,
                "Default confidence due to error",
                "Unable to fully assess perception completeness",
            )

        data = result.unwrap()

        # Parse gaps from validated response
        gaps = []
        for g in data.get("gaps", []):
            gaps.append(PerceptionGap(
                description=g.get("description", "") if isinstance(g, dict) else str(g),
                severity=g.get("severity", "minor") if isinstance(g, dict) else "minor",
                impact=g.get("impact", "") if isinstance(g, dict) else "",
                suggestions=g.get("suggestions", []) if isinstance(g, dict) else [],
            ))

        # Confidence is already calibrated by RobustReasoner
        return (
            gaps,
            data.get("can_proceed", True),
            data.get("proceed_reason", ""),
            data.get("confidence", 0.5),
            data.get("confidence_reasoning", ""),
            data.get("overall_assessment", ""),
        )

    def get_health(self) -> Dict[str, Any]:
        """Get health status of all robustness layers."""
        return self.robust_reasoner.get_health()

    def record_outcome(
        self,
        decision_id: str,
        success: bool,
        description: str = "",
    ) -> None:
        """Record outcome for learning."""
        self.robust_reasoner.record_outcome(decision_id, success, description)


# =============================================================================
# CONVENIENCE FUNCTION
# =============================================================================

def perceive_intelligent(
    goal: Goal,
    reasoner: Optional[Reasoner] = None,
) -> PerceptionResult:
    """
    Convenience function for intelligent perception.

    Usage:
        result = perceive_intelligent(Goal("Query sales data", GoalType.QUERY))
        print(result.summary())
    """
    engine = create_perception_engine(reasoner=reasoner)
    return engine.perceive(goal)


# =============================================================================
# TEST
# =============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(message)s")

    print("=" * 70)
    print("INTELLIGENT PERCEPTION - LLM-First Design")
    print("=" * 70)
    print("\nUsing SimpleReasoner (mock LLM) for testing.")
    print("In production, provide a real Reasoner implementation.\n")

    # Test with mock reasoner
    engine = create_perception_engine()

    # Test 1
    print("=" * 70)
    print("[TEST 1] File creation goal")
    print("=" * 70)

    goal1 = Goal(
        intent="Create a report.txt file with sales data from the database",
        goal_type=GoalType.CREATE
    )

    result1 = engine.perceive(goal1)
    print(f"\n{result1.summary()}")

    # Test 2
    print("\n" + "=" * 70)
    print("[TEST 2] Database query goal")
    print("=" * 70)

    goal2 = Goal(
        intent="Query total revenue from orders table grouped by month",
        goal_type=GoalType.QUERY
    )

    result2 = engine.perceive(goal2)
    print(f"\n{result2.summary()}")

    # Test 3
    print("\n" + "=" * 70)
    print("[TEST 3] Code analysis goal")
    print("=" * 70)

    goal3 = Goal(
        intent="Analyze the authentication module and find security issues",
        goal_type=GoalType.ANALYZE
    )

    result3 = engine.perceive(goal3)
    print(f"\n{result3.summary()}")

    print("\n" + "=" * 70)
    print("[OK] Intelligent Perception Engine Working")
    print("")
    print("KEY DESIGN:")
    print("  - LLM decomposes the goal (not regex)")
    print("  - LLM selects domains to check (not keywords)")
    print("  - LLM filters and scores entities (not rules)")
    print("  - LLM analyzes gaps (not formulas)")
    print("  - LLM decides confidence (not arithmetic)")
    print("=" * 70)


# =============================================================================
# BACKWARD COMPATIBILITY ALIASES
# =============================================================================

# Alias for backward compatibility with loop.py and other code
PerceptionEngine = IntelligentPerceptionEngine

# The old GoalAnalyzer is now replaced by LLM-based decomposition
# No separate analyzer needed - IntelligentPerceptionEngine handles it all
GoalAnalyzer = None  # Deprecated, use IntelligentPerceptionEngine

# The old Perceiver is now RawDataCollector (dumb data gathering)
Perceiver = RawDataCollector
